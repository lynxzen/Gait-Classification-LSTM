{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab2d7f8b",
   "metadata": {},
   "source": [
    "### Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "511ac4f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-04 10:50:35.589227: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-04 10:50:35.812311: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-03-04 10:50:35.812334: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-03-04 10:50:35.813892: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-03-04 10:50:35.930319: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-03-04 10:50:35.931319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-04 10:50:36.906367: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccbaff",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "708f1d9a",
   "metadata": {},
   "source": [
    "## Feature Scaling & File Sorting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f86709f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import os\n",
    "import glob\n",
    "\n",
    "# input and output directories\n",
    "root_dir = '/home/joshuavargas/research/datasets/BB-MAS_Dataset/BB-MAS_Dataset/'\n",
    "output_base_dir = '/home/joshuavargas/research/datasets/BB-MAS_Preprocessed/'\n",
    "\n",
    "# distinguish between accelerometer and gyroscope data\n",
    "sensor_types = {'Accelerometer': 'accelerometer', 'Gyroscope': 'gyroscope'}\n",
    "\n",
    "# folders 1 - 117\n",
    "for i in range(1, 118):\n",
    "    folder_path = os.path.join(root_dir, str(i))\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*_PocketPhone_*.csv'))\n",
    "    \n",
    "    if not csv_files:\n",
    "        continue\n",
    "    \n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        \n",
    "        # copy the features that aren't being normalized then normalize to be between -1 and 1\n",
    "        output_df = df[['time']].copy()\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        \n",
    "        for column in ['Xvalue', 'Yvalue', 'Zvalue']:\n",
    "            scaled_values = scaler.fit_transform(df[[column]])\n",
    "            output_df[column] = scaled_values\n",
    "            \n",
    "        # convert timestamp to panda datetime -> calculate elapsed times -> drop original timestamp\n",
    "        output_df['time'] = pd.to_datetime(output_df['time'])\n",
    "        output_df['elapsed_time'] = output_df['time'].diff().dt.total_seconds().fillna(0)\n",
    "        output_df.drop(columns=['time'], inplace=True)\n",
    "        \n",
    "        # determine sensor type for output directory\n",
    "        for sensor_key, sensor_folder in sensor_types.items():\n",
    "            if sensor_key in csv_file:\n",
    "                sensor_output_dir = os.path.join(output_base_dir, sensor_folder)\n",
    "                break\n",
    "        else:\n",
    "            sensor_output_dir = os.path.join(output_base_dir, 'other')\n",
    "        \n",
    "        # make sure folders exist and set output file names to be the input file name\n",
    "        os.makedirs(sensor_output_dir, exist_ok=True)\n",
    "        output_filename = os.path.basename(csv_file)\n",
    "        \n",
    "        # Write to file in the specific sensor type folder\n",
    "        output_df.to_csv(os.path.join(sensor_output_dir, output_filename), index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa11735f",
   "metadata": {},
   "source": [
    "## Resizing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7c1b6c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Least number of rows in accelerometer folder:  17024\n",
      "Least number of rows in gyroscope folder:  17023\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def resize_files_in_folder(folder_path, sensor_type):\n",
    "    csv_files = glob.glob(os.path.join(folder_path, '*_PocketPhone_*.csv'))\n",
    "    num_rows = 1000000  # Start with a large number that will be reduced to the smallest file size\n",
    "\n",
    "    # First pass: Find the minimum number of rows\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        num_rows = min(num_rows, df.shape[0])\n",
    "\n",
    "    print(f\"Least number of rows in {sensor_type} folder: \", num_rows)\n",
    "\n",
    "    # Second pass: Resize files\n",
    "    for csv_file in csv_files:\n",
    "        df = pd.read_csv(csv_file)\n",
    "        resized_df = df.head(num_rows)  # Keep only the top 'num_rows' rows\n",
    "        resized_df.to_csv(csv_file, index=False)  # Save the resized DataFrame back to the same file\n",
    "\n",
    "# Paths to accelerometer and gyroscope data folders\n",
    "folder_path_accelerometer = '/home/joshuavargas/research/datasets/BB-MAS_Preprocessed/accelerometer/'\n",
    "folder_path_gyroscope = '/home/joshuavargas/research/datasets/BB-MAS_Preprocessed/gyroscope/'\n",
    "\n",
    "# Process accelerometer and gyroscope data\n",
    "resize_files_in_folder(folder_path_accelerometer, \"accelerometer\")\n",
    "resize_files_in_folder(folder_path_gyroscope, \"gyroscope\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de30f6d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1168269f",
   "metadata": {},
   "source": [
    "## Sequencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "124bbe80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'your_label_logic_here' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     X[i] \u001b[38;5;241m=\u001b[39m features[i\u001b[38;5;241m*\u001b[39msequence_length:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming y is a numpy array with your labels\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([your_label_logic_here \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sequences)])\n",
      "Cell \u001b[0;32mIn[2], line 19\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     16\u001b[0m     X[i] \u001b[38;5;241m=\u001b[39m features[i\u001b[38;5;241m*\u001b[39msequence_length:(i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m*\u001b[39msequence_length]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Assuming y is a numpy array with your labels\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([your_label_logic_here \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_sequences)])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'your_label_logic_here' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7f451d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features:\n",
      " [[[0.65307536 0.22328091 0.58793919 0.        ]\n",
      "  [0.64372309 0.27365402 0.56851849 0.011     ]\n",
      "  [0.6228413  0.32416043 0.54596388 0.009     ]]\n",
      "\n",
      " [[0.64372309 0.27365402 0.56851849 0.011     ]\n",
      "  [0.6228413  0.32416043 0.54596388 0.009     ]\n",
      "  [0.60794695 0.36380595 0.53452039 0.01      ]]\n",
      "\n",
      " [[0.6228413  0.32416043 0.54596388 0.009     ]\n",
      "  [0.60794695 0.36380595 0.53452039 0.01      ]\n",
      "  [0.60616556 0.39132461 0.52958212 0.01      ]]\n",
      "\n",
      " [[0.60794695 0.36380595 0.53452039 0.01      ]\n",
      "  [0.60616556 0.39132461 0.52958212 0.01      ]\n",
      "  [0.61205403 0.40018655 0.51661915 0.01      ]]\n",
      "\n",
      " [[0.60616556 0.39132461 0.52958212 0.01      ]\n",
      "  [0.61205403 0.40018655 0.51661915 0.01      ]\n",
      "  [0.6303627  0.39012525 0.50650519 0.009     ]]]\n",
      "Target: [0.54596388 0.53452039 0.52958212 0.51661915 0.50650519]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "root_dir = '/home/joshuavargas/research/datasets/BB-MAS_Preprocessed/accelerometer/1_PocketPhone_Accelerometer_(Samsung_S6).csv'\n",
    "df = pd.read_csv(root_dir)\n",
    "\n",
    "# Assuming you want to predict Zvalue\n",
    "features_df = df[['Xvalue', 'Yvalue', 'Zvalue', 'elapsed_time']]\n",
    "targets_df = df['Zvalue']\n",
    "\n",
    "# Convert DataFrame and Series to TensorFlow datasets\n",
    "features_dataset = tf.data.Dataset.from_tensor_slices(features_df.values)\n",
    "targets_dataset = tf.data.Dataset.from_tensor_slices(targets_df.values)\n",
    "\n",
    "window_size = 3\n",
    "batch_size = 5\n",
    "\n",
    "# Create a dataset of windows\n",
    "dataset = tf.data.Dataset.zip((features_dataset, targets_dataset))\n",
    "dataset = dataset.window(window_size, shift=1, drop_remainder=True)\n",
    "dataset = dataset.flat_map(lambda f, t: tf.data.Dataset.zip((f.batch(window_size), t.batch(window_size))))\n",
    "dataset = dataset.map(lambda f, t: (f, t[-1]))\n",
    "dataset = dataset.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "for features, target in dataset.take(1):\n",
    "    print(\"Features:\\n\", features.numpy())\n",
    "    print(\"Target:\", target.numpy())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d970f4",
   "metadata": {},
   "source": [
    "## Scale Verification "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e7df0697",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m root_dir   \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/joshuavargas/research/datasets/BB-MAS_Dataset/BB-MAS_Dataset/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/Users/joshuavargas/research/datasets/BB-MAS_Preprocessed/accelerometer/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 6\u001b[0m folder_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(root_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      7\u001b[0m csv_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(folder_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m*_PocketPhone_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[1;32m      8\u001b[0m scaled_files \u001b[38;5;241m=\u001b[39m glob\u001b[38;5;241m.\u001b[39mglob(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1_PocketPhone_*.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root_dir   = '/Users/joshuavargas/research/datasets/BB-MAS_Dataset/BB-MAS_Dataset/'\n",
    "output_dir = '/Users/joshuavargas/research/datasets/BB-MAS_Preprocessed/accelerometer/'\n",
    "\n",
    "folder_path = os.path.join(root_dir, \"1\")\n",
    "csv_files = glob.glob(os.path.join(folder_path, '*_PocketPhone_*.csv'))\n",
    "scaled_files = glob.glob(os.path.join(output_dir, '1_PocketPhone_*.csv'))\n",
    "\n",
    "df = pd.read_csv(csv_files[0])\n",
    "X_train = df['Xvalue']\n",
    "X_scatter = df[['Xvalue', 'Yvalue']]\n",
    "\n",
    "df_scaled = pd.read_csv(scaled_files[0])\n",
    "X_train_scaled = df_scaled['Xvalue']\n",
    "X_scatter_scaled = df[['Xvalue', 'Yvalue']]\n",
    "\n",
    "# histograms\n",
    "\n",
    "# before scaling\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(X_train, bins=50, alpha=0.5, label='Before Scaling')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# after scaling\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(X_train_scaled, bins=50, alpha=0.5, label='After Scaling')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# scatter plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_scatter['Xvalue'], X_scatter['Yvalue'], alpha=0.5, label='Before Scaling')\n",
    "plt.scatter(X_scatter_scaled['Xvalue'], X_scatter_scaled['Yvalue'], alpha=0.5, label='After Scaling')\n",
    "plt.title(\"Feature Comparison Before and After Scaling\")\n",
    "plt.xlabel(\"Xvalue\")\n",
    "plt.ylabel(\"Yvalue\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bfc0de2",
   "metadata": {},
   "source": [
    "# LSTM Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c85b4b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
